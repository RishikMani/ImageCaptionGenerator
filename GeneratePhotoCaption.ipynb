{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a deep learning photo caption generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I will be developing a deep learning model which is able to generate textual description for a given photograph. For such a task, our model needs to utilize features learned from various images using computer vision and understanding the natural language as well. The task is to produce a meaningful caption given an image.\n",
    "\n",
    "I have trained my model using GPU and I will be sharing my environment configuration as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photo and caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I will be using Flickr8k dataset. It consists of 8001 images and per image it has 5 captions. These captions describe the entities and the events in the image very clearly. The dataset can be downloaded from my Github repository. Download the datasets and then unzip them in your current working directory. You will have two directories:\n",
    "<ul>\n",
    "    <li>Flicker8k_Dataset: It consists of 8091 images.</li>\n",
    "    <li>Flickr8k_text: It contains a number of file describing the dataset.</li>\n",
    "</ul>\n",
    "\n",
    "The dataset has been defined and divided into 6000 training images, 1000 development images and 1000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare photo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using a pre-trained model to extract features from different images. For this, I will use VGG16 model which is provided as part of Keras. If you do not have the model already, then the model would be downloaded on running the code. Depending on your internet connectivity and bandwidth this might take time. The download size is roughly 500 megabytes.\n",
    "\n",
    "For this task, I am only interested in finding the features of the images and so I will remove the output layer (last layer) from the model. These features would serve as one the input features for my caption generator model. Once I have all the features for all the images I will dump them as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "def extract_features(directory):\n",
    "    # create an instance of VGG 16 model.\n",
    "    # if this model is not already downloaded, it would first be downloaded\n",
    "    model = VGG16()\n",
    "\n",
    "    # we create a new model with the same input shape as the VGG model,\n",
    "    # but we are not interested in the output of the model\n",
    "    # we are rather interested in the features just before the output layer\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    model.summary()\n",
    "\n",
    "    # a directory to store the features from all the images\n",
    "    # key: image id, value: image feature\n",
    "    features = dict()\n",
    "    for image in listdir(directory):\n",
    "        # unique image identifier\n",
    "        image_id = image.split(\".\")[0]\n",
    "\n",
    "        # load the image and preprocess it to VGG16 input format\n",
    "        # VGG imposes restrictions on the image size and expects image to be of size 224x224\n",
    "        image = load_img(directory + \"/\" + image, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "\n",
    "        # VGG16 expects image to be in BRG format rather than RGB format\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[image_id] = feature\n",
    "    return features\n",
    "\n",
    "features = extract_features(\"Flicker8k_Dataset\")\n",
    "# store the features as a dictionary in pickle format\n",
    "dump(features, open(\"features.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file Flickr8k.token.txt inside the folder Flickr8k_text contains descriptions about the images. Each line has a fomrat _imageName_#__commentNumber _description_. We will create a dictionary containing the list of all descriptions for a given image Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    return content\n",
    "\n",
    "# load the content of the file\n",
    "doc = load_doc(\"Flickr8k_text/Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split(\"\\n\"):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0].split(\".\")[0], tokens[1:]\n",
    "        image_desc = \" \".join(image_desc)\n",
    "        \n",
    "        # create a dictionary item, such that the key is the image id\n",
    "        # and the value is the list of all descriptions for a given image\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "descriptions = load_descriptions(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the image descriptions have been loaded it's time to pre-process and clean our text. This process involves:\n",
    "<ol>\n",
    "    <li>Converting all text to lowercase</li>\n",
    "    <li>Removing all punctuation marks</li>\n",
    "    <li>Removing one letter words like \"a\", hanging s ('s), etc.</li>\n",
    "    <li>Freeing the text of all numbers</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for _, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] = \" \".join(desc)\n",
    "\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the following code is not mandatory for this task, but it gives an approximation, as to how big the vocabulary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(desc.split()) for desc in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the descriptions in a file such that each line consists of an image Id and a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + \" \" + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_descriptions(descriptions, \"descriptions.txt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pre-defined set of identifiers given a file name. As we have three different files for train, dev and test, we have three sets of image identifiers. The ```load_set``` function will return the list of all unique image identifiers for a given type of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    return doc\n",
    "\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split(\"\\n\"):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        image_id = line.split(\".\")[0]\n",
    "        dataset.append(image_id)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify when a string starts and ends, we process each description belonging to the training set to contain _startseq_ as the beginning identifier and _endseq_ as the last identifier. This would be very necessary for our caption generation, as we will produce caption word by word and will stop once the _endseq_ identifier is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split(\"\\n\"):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            image_desc = \"startseq \" + \" \".join(image_desc) + \" endseq\"\n",
    "            descriptions[image_id].append(image_desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```load_photo_features``` function is used to load the _features.pkl_ file and return a dictionary containing all pairs of image identifier and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, \"rb\"))\n",
    "    features = {k: all_features[k] for k in dataset}  ## why feature returned is a dictionary\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset before we start to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "filename = \"Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "train_features = load_photo_features(\"features.pkl\", train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenizer and fit it on the training descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(desc) for desc in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# save the tokenizer in pickle format\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of the longest description\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```create_sequences``` takes in a tokenizer, the maximum sequence length, a list of all the descriptions, a photo feature and the vocabulary size as input argumenta. It returns an input output pair of training data for our model. Our model has two inputs: photo feature and the encoded text and outputs the next word in the sequence. We will keep predicting the next word in the sequence until we meet the end sequence identifier, i.e. _endseq_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    for desc in desc_list:\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```data_generator``` is used to create training sets with progressive loading. I am training my model using NVIDIA GTX 970 with 4GB VRAM, which is not sufficient for our task. Rather than loading all the data at once, we will create batches of 1 training set and train progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is actually a merge model. A merge model means, it merges the output of two models together, feeds them as an input into a new layer and finally gets an output. In the following section you can see we have two _inputs_. \n",
    "1. The first input (_input1_) is going to be the features extracted from our VGG model which would then be fed to a dense layer consisting of 256 neurons.\n",
    "2. The second input (_input2_) is going to be a input sequence derived from the descriptions with the maximum length of the lonngest description. This input is fed to an embedding layer which generates embeddings, which are then fed into an LSTM layer. The output of the LSTM layer is then fed to a dense layer consisting of 256 neurons.\n",
    "3. Before the outputs from the above two models is fed into the dense layer, the outputs are combined into a single representation using _add_ layer.\n",
    "4. Finally, the output of the first dense layer is fed into a new dense layer that makes a softmax prediction over the entire vocabulary for the next word in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, LSTM, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(vocab_size, length):\n",
    "    # feature extracted from VGG16 model\n",
    "    input1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(input1)\n",
    "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "    \n",
    "    # generate embeddings for the sequences and feed them to LSTM\n",
    "    input2 = Input(shape=(length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # combine the output of the above two models before feeding forward to the dense layer\n",
    "    decoder1 = add([fe2, se3])\n",
    "    \n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "\n",
    "# load clean descriptions for the training set\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "\n",
    "# load training set image features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "# tokenize the training set descriptions\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# determine the length of the longest description in training set\n",
    "length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = define_model(vocab_size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishik/miniconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 405s 67ms/step - loss: 5.1716\n",
      "   1/6000 [..............................] - ETA: 6:32 - loss: 4.3799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishik/miniconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 318s 53ms/step - loss: 4.3716\n",
      "6000/6000 [==============================] - 319s 53ms/step - loss: 4.0866\n",
      "6000/6000 [==============================] - 319s 53ms/step - loss: 3.9042\n",
      "6000/6000 [==============================] - 319s 53ms/step - loss: 3.7775\n",
      "6000/6000 [==============================] - 342s 57ms/step - loss: 3.6575\n",
      "6000/6000 [==============================] - 358s 60ms/step - loss: 3.5774\n",
      "6000/6000 [==============================] - 329s 55ms/step - loss: 3.4875\n",
      "6000/6000 [==============================] - 402s 67ms/step - loss: 3.4149\n",
      "6000/6000 [==============================] - 335s 56ms/step - loss: 3.3515\n",
      "6000/6000 [==============================] - 345s 57ms/step - loss: 3.3034\n",
      "6000/6000 [==============================] - 343s 57ms/step - loss: 3.2546\n",
      "6000/6000 [==============================] - 341s 57ms/step - loss: 3.2238\n",
      "6000/6000 [==============================] - 328s 55ms/step - loss: 3.1734\n",
      "6000/6000 [==============================] - 321s 54ms/step - loss: 3.1427\n",
      "6000/6000 [==============================] - 322s 54ms/step - loss: 3.1204\n",
      "6000/6000 [==============================] - 320s 53ms/step - loss: 3.0822\n",
      "6000/6000 [==============================] - 320s 53ms/step - loss: 3.0600\n",
      "6000/6000 [==============================] - 350s 58ms/step - loss: 3.0358\n",
      "6000/6000 [==============================] - 323s 54ms/step - loss: 3.0221\n"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, length, vocab_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate my model on the held out testing dataset. We will generate descriptions for the testing dataset and evaluate those predictions with a standard cost function. For each description generation, our start token would be _startseq_ and would run until the token _endseq_ is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return back the original word for a given token id\n",
    "def detokenize(tokenizer, integer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def generate_description(model, photo, tokenizer, length):\n",
    "    # start sequence for our string\n",
    "    in_text = \"startseq\"\n",
    "\n",
    "    # a sequence can have maximum \"length\" number of words,\n",
    "    # thus iterate over the max possible length\n",
    "    for i in range(length):\n",
    "        # tokenize the input text sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the input to a maximum length of the longest description\n",
    "        sequence = pad_sequences([sequence], maxlen=length)\n",
    "        # predict the next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = argmax(yhat)\n",
    "        # map the integer to the word\n",
    "        word = detokenize(tokenizer, yhat)\n",
    "        # if text could not be predicted then stop\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        # if the end sequence token has been generated then stop\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using BLEU scores to compare the qulaity of text generated using the model with the actual textual descriptions. Wikipedia says BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_model(model, descriptions, photo, tokenizer, length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_description(model, photo[key], tokenizer, length)\n",
    "        references = [desc.split() for desc in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then test the model on the heldout testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.417263\n",
      "BLEU-2: 0.178178\n",
      "BLEU-3: 0.108978\n",
      "BLEU-4: 0.043869\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "\n",
    "# test set descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "\n",
    "# test set photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = \"model_19.h5\"\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_for_single_image(image):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "    image = load_img(image, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    feature = model.predict(image)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq two dogs run through the water endseq\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load(open(\"tokenizer.pkl\", \"rb\"))\n",
    "features = extract_feature_for_single_image(\"example.jpg\")\n",
    "description = generate_description(model, features, tokenizer, length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this task I referred to Jason Brownlee's [machine learning blog](machinelearningmastery.com). The blog could be found [here](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2486159694cae14c695fffd06dba646216ae2c1a06c7ee9915e8086bf5d10cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
